{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e80299",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <center>\n",
    "        Implementation of IA for autonomous vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d97fd0",
   "metadata": {},
   "source": [
    "# Step 1 – Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23808a29",
   "metadata": {},
   "source": [
    "1. os: The operating system library, used to load the saved AI models.\n",
    "\n",
    "2. random: Used to sample some random transitions from the memory for experience replay.\n",
    "\n",
    "3. torch: The main library from PyTorch, which will be used to build our neural network with tensors, as opposed to simple matrices like numpy arrays. While a matrix is a 2-D array, a tensor can be a n-dimensional array, with more than just a single number in its cells. Here's a diagram so you can clearly understand the difference between a matrix and a tensor:\n",
    "\n",
    "4. torch.nn: The nn module from the torch library, used to build the fully connected layers in the artificial neural network of our AI.\n",
    "5. torch.nn.functional: The functional sub-module from the nn module, used to call the activation functions (rectifier and Softmax), as well as the loss function for backpropagation.\n",
    "\n",
    "6. torch.optim: The optim module from the torch library, used to call the Adam optimizer, which computes the gradients of the loss with respect to the weights and updates those weights in directions that reduce the loss.\n",
    "\n",
    "7. torch.autograd: The autograd module from the torch library, used to call the Variable class, which associates each tensor and its gradient into the same variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c22b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\callidus\\anaconda3\\envs\\Callidus-Deep-Q\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# AI for Autonomous Vehicles - Build a Self-Driving Car\n",
    "\n",
    "# Importing the libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb8701",
   "metadata": {},
   "source": [
    "# Step 2 – Creating the architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2179ce",
   "metadata": {},
   "source": [
    "First, you build this brain inside a class, which we are going to call Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c75966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 30)\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005d0a8",
   "metadata": {},
   "source": [
    "# Step 3 – Implementing experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0a093",
   "metadata": {},
   "source": [
    "Time for the next step! You'll now build another class, which builds the memory object for experience replay (as seen in Chapter 5, Your First AI Model – Beware the Bandits!). Call this class ReplayMemory. Let's have a look at the code first and then I'll explain everything line by line from the deep_q_learning.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9064e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Experience Replay\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbbced3",
   "metadata": {},
   "source": [
    "# Step 4 – Implementing deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34a938",
   "metadata": {},
   "source": [
    "You've made it! You're finally about to start coding the whole deep Q-learning process. Again, you'll wrap all of it into a class, this time called Dqn, as in deep Q-network. This is your final run before the finish line. Let's smash this.\n",
    "\n",
    "This time, the class is quite long so I'll show and explain the lines of code method by method from the deep_q_learning.py file. Here's the first one, the __init__() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52388d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Deep Q-Learning\n",
    "\n",
    "class Dqn(object):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(capacity = 100000)\n",
    "        self.optimizer = optim.Adam(params = self.model.parameters())\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19d1b7",
   "metadata": {},
   "source": [
    "Now, you're all good for the __init__ method. Let's move on to the next code section with the next method: the select_action method, which selects the action to play at each iteration using Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c63da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def select_action(self, state):\n",
    "        probs = F.softmax(self.model(Variable(state))*100)\n",
    "        action = probs.multinomial(len(probs))\n",
    "        return action.data[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9735110",
   "metadata": {},
   "source": [
    "Let's move on to the next code section, the learn method. This one is pretty interesting because it's where the heart of deep Q-learning beats. It's in this method that we compute the temporal difference, and accordingly the loss, and update the weights with our optimizer in order to reduce that loss. That's why this method is called learn, because it is here that the AI learns to perform better and better actions that increase the accumulated reward. Let's continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15a76f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learn(self, batch_states, batch_actions, batch_rewards, batch_next_states):\n",
    "        batch_outputs = self.model(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze(1)\n",
    "        batch_next_outputs = self.model(batch_next_states).detach().max(1)[0]\n",
    "        batch_targets = batch_rewards + self.gamma * batch_next_outputs\n",
    "        td_loss = F.smooth_l1_loss(batch_outputs, batch_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7801c",
   "metadata": {},
   "source": [
    "Congratulations! You've built yourself a tool in the Dqn class that will train your car to drive better. You've done the toughest part. Now all you have left to do is to wrap things up into a last method, called update, which will simply update the weights after reaching a new state.\n",
    "\n",
    "Now, in case you are thinking, \"but isn't what I've already done with the learn method?,\" well, you're right; but you need to make an extra function that will update the weights at the right time. The right time to update the weights is right after our AI reaches a new state. Simply put, this final update method you're about to implement will connect the dots between the learn method and the dynamic environment.\n",
    "\n",
    "That's the finish line! Are you ready? Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8562ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, new_state, new_reward):\n",
    "        new_state = torch.Tensor(new_state).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward]), new_state))\n",
    "        new_action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_states, batch_actions, batch_rewards, batch_next_states = self.memory.sample(100)\n",
    "            self.learn(batch_states, batch_actions, batch_rewards, batch_next_states)\n",
    "        self.last_state = new_state\n",
    "        self.last_action = new_action\n",
    "        self.last_reward = new_reward\n",
    "        return new_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ea83c",
   "metadata": {},
   "source": [
    "These last two methods aren't AI-related, so we won't spend time explaining each line of their code. Still, it's good for you to be able to recognize these two tools in case you want to use them for another AI model that you build with PyTorch.\n",
    "\n",
    "Here's how they're implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "695bbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b5d6f",
   "metadata": {},
   "source": [
    "# Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509387b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI for Autonomous Vehicles - Build a Self-Driving Car\n",
    "\n",
    "# Importing the libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Creating the architecture of the Neural Network\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 30)\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values\n",
    "\n",
    "# Implementing Experience Replay\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)\n",
    "\n",
    "# Implementing Deep Q-Learning\n",
    "\n",
    "class Dqn(object):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(capacity = 100000)\n",
    "        self.optimizer = optim.Adam(params = self.model.parameters())\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        probs = F.softmax(self.model(Variable(state))*100)\n",
    "        action = probs.multinomial(len(probs))\n",
    "        return action.data[0,0]\n",
    "    \n",
    "    def learn(self, batch_states, batch_actions, batch_rewards, batch_next_states):\n",
    "        batch_outputs = self.model(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze(1)\n",
    "        batch_next_outputs = self.model(batch_next_states).detach().max(1)[0]\n",
    "        batch_targets = batch_rewards + self.gamma * batch_next_outputs\n",
    "        td_loss = F.smooth_l1_loss(batch_outputs, batch_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update(self, new_state, new_reward):\n",
    "        new_state = torch.Tensor(new_state).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward]), new_state))\n",
    "        new_action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_states, batch_actions, batch_rewards, batch_next_states = self.memory.sample(100)\n",
    "            self.learn(batch_states, batch_actions, batch_rewards, batch_next_states)\n",
    "        self.last_state = new_state\n",
    "        self.last_action = new_action\n",
    "        self.last_reward = new_reward\n",
    "        return new_action\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "    \n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73663642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
